# -*- coding: utf-8 -*-
"""Architecture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lpc5q2Bvms_B7-Cg5OfBg0kAvo1tX5nq
"""

#Installing the required items
!pip install sentencepiece
!pip install transformers

#Imporing the necessary items
import nltk
import numpy as np
import re
from gensim.models import Word2Vec
from sklearn.cluster import KMeans
from scipy.spatial import distance
#nltk.download('punkt')   # one time execution
#nltk.download('stopwords')  # one time execution
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
# 
#sentence tokenization

from nltk.tokenize import sent_tokenize
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense,Dropout, Input, Concatenate
from tensorflow.keras import regularizers
from transformers import *
from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig

import nltk
nltk.download('punkt')   # one time executionfrom nltk.tokenize import sent_tokenize
#sentence = sent_tokenize(text)

import pandas as pd
df=pd.read_csv('input.csv')   #Reading the input files
df['Label']=df.Label.replace('',np.nan).astype(float)   #Refreshing the input file
df.tail()

#Model Definition
dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')


#You can make it false,if you donot need fine tuning of DistilBERT
for layer in dbert_model.layers:
    layer.trainable = True

max_len=50

#Model Definition
def create_model():
    inps = Input(shape = (max_len,), dtype='int64')  #=>.x
    masks= Input(shape = (max_len,), dtype='int64')  #=>y
    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]
    #freq_layer=dbert_layer
    freq_layer = Input(shape = (len(dbert_tokenizer.vocab)-1), dtype='float64') # the first token (PAD token) is not used in the tokens frequencies
    dense0 = Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.01))(freq_layer)
    dropout0= Dropout(0.5)(dense0)
    #concatted=dbert_layer
    concatted = Concatenate()([dbert_layer, dropout0])
    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(concatted)
    dropout= Dropout(0.5)(dense)
    pred = Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(dropout) #For model in the Paper, replace sigmoid by relu.
    model = tf.keras.Model(inputs=[inps,masks,freq_layer], outputs=pred)
    print(model.summary())
    return model

# Tokenizer vocabulary update (add htg, mtn, url and rtw tokens)

dbert_tokenizer.vocab["htg"]=len(dbert_tokenizer.vocab)
dbert_tokenizer.vocab["mtn"]=len(dbert_tokenizer.vocab)
dbert_tokenizer.vocab["url"]=len(dbert_tokenizer.vocab)
dbert_tokenizer.vocab["rtw"]=len(dbert_tokenizer.vocab)

dbert_model.resize_token_embeddings(len(dbert_tokenizer))
print(len(dbert_tokenizer.vocab))

#Preparing the inputs
input_ids=[]
attention_masks=[]
sentences=list(df['text'])
labels=list(df['Label'])
# for i in sentences:
#   print(i)
#print(sentences)
an_array=[]
for sent in df['text']:
    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)
    input_ids.append(dbert_inps['input_ids'])
    attention_masks.append(dbert_inps['attention_mask'])
    
n=len(input_ids)
an_array=[]
for i in range(n):
  res=[]
  for k in range(30525):
    res.append(0);
  for j in input_ids[i]:
    if j != 0:
      res[j-1]+=1;
  an_array.append(res)

x=np.asarray(input_ids) 
y=np.array(attention_masks)
an_array=np.array(an_array)
labels=np.array(labels)
len(x),len(y),len(labels)
print(an_array)

#Create an instance of model
model=create_model()

#Compiling the model.For the paper model replace binarycrossentropy loss by mean squared error.
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5,beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam'),loss=tf.keras.losses.BinaryCrossentropy()
,metrics=[tf.keras.metrics.MeanSquaredError(name="mean_squared_error")])
# gradients = tape.gradient(loss, model.trainable_variables, 
#                 unconnected_gradients=tf.UnconnectedGradients.ZERO)

#model_save_path = '/content/drive/MyDrive/Model/model_Fold1_5epochs.h5' # model_save_path is the path to a file with pretrained weights (e.g. one of the .h5 files in the Model folder)
#model.load_weights(model_save_path)

model.fit([x,y,an_array],[labels],batch_size=8,epochs=3) #training

# model.save_weights("model.h5")

# model.load_weights('model.h5')

#Loading Test Data
testing=[] 
filename="input.txt"
f = open(filename, encoding="utf8")
for x in f:
	testing.append(x)
print(testing)

pred_inp=[]
pred_masks=[]
#sentences=testing
#labels=list(df['Label'])
# for i in sentences:
#   print(i)
#print(sentences)
an_array=[]
for sent in testing:
    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)
    pred_inp.append(dbert_inps['input_ids'])
    pred_masks.append(dbert_inps['attention_mask'])
n=len(pred_inp)
an_array=[]
i=0
for i in range(n):
  res=[]
  for l in range(30525):
    res.append(0);
  for j in pred_inp[i]:
    if j != 0:
      res[j-1]+=1;
  an_array.append(res)

xpred=np.asarray(pred_inp)
ypred=np.array(pred_masks)
#labels=np.array(labels)
len(xpred),len(ypred)

# n=len(xpred)
# an_array=[]
# for i in range(n):
#   an_array.append(res)

an_array=np.array(an_array)
# an_array=an_array.reshape(1,30525)
# n=len(x)
# an=[]
# for i in range(n):
#   an.append(an_array)
print(an_array)
preds = model.predict([xpred,ypred,an_array],batch_size=1)
pred_labels = preds.argmax(axis=1)

preds[0:2]

##For Hagupit
df2=pd.DataFrame()
df2=df2.assign(text=testing)
df2 = df2.assign(Score=preds)
df2 = df2.sort_values(by = 'Score')
sol=[]
i=len(df2['text'])-1
cp=0
while cp < 41:
  sol.append(df2['text'][i])
  i-=1
  cp+=1
print(sol)
#df2.head()

from google.colab import drive
drive.mount('/content/drive')

!pip install py-rouge

#Rouge 1 F1 Score calculation
######################
#ROUGE calculation
######################

i_t=""
for i in range(len(sol)):
    i_t=i_t + " " + sol[i]
        
input_t=[]
input_t.append(i_t)


Summary_tweet=[]  
f = open("Annotator1.txt", "r", encoding="utf8")
for x in f:
    Summary_tweet.append(x)

g_t=""
for i in range(len(Summary_tweet)):
    p=Summary_tweet[i].split('\n')
    g_t=g_t + " " + p[0]

ground_truth=[]
ground_truth.append(g_t)


import rouge
evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],
                           max_n=4,
                           limit_length=True,
                           length_limit=1000,
                           length_limit_type='words',
                           apply_avg=True,
                           #apply_best=apply_best,
                           alpha=0.5, # Default F1_score
                           weight_factor=1.2,
                           stemming=True)

ROUGEscores1 = evaluator.get_scores(input_t, ground_truth)


Summary_tweet=[]  
f = open("Annotator2.txt", "r", encoding="utf8")
for x in f:
    Summary_tweet.append(x)

g_t=""
for i in range(len(Summary_tweet)):
    p=Summary_tweet[i].split('\n')
    g_t=g_t + " " + p[0]

ground_truth=[]
ground_truth.append(g_t)

ROUGEscores2 = evaluator.get_scores(input_t, ground_truth)


Summary_tweet=[]  
f = open("Annotator3", "r", encoding="utf8")
for x in f:
    Summary_tweet.append(x)

g_t=""
for i in range(len(Summary_tweet)):
    p=Summary_tweet[i].split('\n')
    g_t=g_t + " " + p[0]

ground_truth=[]
ground_truth.append(g_t)

ROUGEscores3 = evaluator.get_scores(input_t, ground_truth)

#print(ROUGEscores3)
mlist=[]
for i in ['rouge-1', 'rouge-2','rouge-l']:
    m=[]
    for j in ['f', 'p', 'r']:
        m1= (ROUGEscores1[i][j]+ROUGEscores2[i][j]+ROUGEscores3[i][j])/3.0
        m.append(m1)
    mlist.append(m)
    #break
print(mlist)
#print(f1_score)


# //Rouge-1 's F1 score,precision,recall.
# //Rouge-2 '  F1 score,precison,recall
# //Rouge-l ,  F1 score,precision,recall
